# 神经网络中的前向传播:组件和应用

> 原文:[https://blog . quantin STI . com/forward-propagation-neural-networks/](https://blog.quantinsti.com/forward-propagation-neural-networks/)

由[瓦伦·迪瓦卡尔](https://www.linkedin.com/in/varun-divakar-b862a667/)和[雷基特·帕查内卡尔](https://www.linkedin.com/in/rekhit/)

让我们直接进入主题。到底什么是神经网络中的前向传播？如果你把这些词分开，forward 意味着向前移动，propagation 是一个表示任何事物传播的术语。前向传播意味着我们在神经网络中只向一个方向移动，从输入到输出。把它想象成穿越时间，在那里我们别无选择，只能勇往直前，只希望我们的错误不会回来困扰我们。

现在，如果你认为神经网络在交易中用处很小，那么我们必须告诉你，几乎所有的量化对冲基金都从神经网络转向了深度学习和人工智能，以某种方式保持对其他人的优势。从文艺复兴理工到两个适马，神经网络正以前所未有的方式被利用。

现在，在我们开始一个前向传播的例子之前，让我们看看这个博客中涉及的主题。

*   [神经网络简史](#a-brief-history-of-neural-networks)
*   [什么是神经网络中的前向传播？](#what-is forward-propagation-in-neural-networks)
*   [正向传播模型的组件](#components-of-forward-propagation-model)
*   [正向传播的应用](#applications-of-forward-propagation)

* * *

## 神经网络简史

我们试图理解自古以来人类是如何工作的。事实上，甚至哲学也在试图理解人类的思维过程。但是，直到最近几年，我们才开始在理解我们的大脑如何运作方面取得进展。这也是传统计算机不同于人类的地方。

你看，虽然我们可以开发一个算法来解决一个问题，但我们必须确保我们已经考虑到了各种可能性。然而，当涉及到人类时，我们可能会从有限或不完整的信息开始，但我们会更快、更准确地“学习”和解决问题。嗯，至少大家都是这么说的！

于是，我们开始研究开发人工大脑，其实现在叫神经网络。

神经网络的基本结构是感知器，它模仿我们细胞中的神经元。

<figure class="kg-card kg-image-card kg-width-full kg-card-hascaption">![basic structure in the neural network](../Images/9233266b1598c79e522f48939793931c.png)

<figcaption>Basic structure in the Neural Network</figcaption>

</figure>

用黄色圆圈标记的是神经元的输入，经过一些计算后，神经元发出输出信号。

输入层类似于神经元的树突，输出信号是轴突。每个输入信号被分配一个权重 wi。这个权重乘以输入值，神经元存储所有输入变量的加权和。

然后将激活函数应用于加权和，这产生神经元的输出信号。

神经网络的一个流行的例子是图像识别软件，它可以识别人脸，并且能够在不同的照明条件下标记同一个人。话虽如此，现在让我们更详细地理解正向传播。

* * *

## 什么是神经网络中的前向传播？

第一个神经网络使用了前向传播的概念。我将借助一个简单的直线方程来解释正向传播。

我们都知道，一条线可以借助于方程来表示:y = mx + b

其中 y 是该点的 y 坐标，m 是斜率，x 是 x 坐标，b 是 y 截距，即直线与 y 轴相交的点。

但是为什么我们要在这里写直线方程呢？这将有助于我们稍后详细了解神经网络的组件。

还记得我们说过神经网络应该模仿人类的思维过程吗？好吧，让我们假设我们不知道直线的方程，但是我们有一张图纸，在上面随机画一条直线。

在这个例子中，你画了一条穿过原点的线，当你看到 x 和 y 坐标时，它们看起来像这样:

<figure class="kg-card kg-image-card kg-width-full kg-card-hascaption">![x and y coordinates](../Images/09524f37798131d2ea6ac9f988d3086d.png)

<figcaption>x and y coordinates</figcaption>

</figure>

这看起来很眼熟。如果我让你找出 x 和 y 之间的关系，你会直接说是 y = 3x。但是让我们来看看正向传播是如何工作的。

这里我们假设 x 是输入，y 是输出。

这里的第一步是参数的初始化。我们会猜测 y 一定是 x 的乘法因子，所以我们假设 y = 5x，然后看结果。让我们把这个加到表上，看看我们离答案还有多远。

<figure class="kg-card kg-image-card kg-width-full">![x and y coordinates and random guess](../Images/b6b275c618e9365e40406e4770b9e1e7.png)</figure>

请注意，取数字 5 只是一个随机的猜测，别无其他。我们可以在这里取任何一个数字。我应该指出，这里我们可以将 5 项作为模型的权重。

好了，这是我们的第一次尝试，现在我们将看看我们离实际输出有多近(或多远)。

一种方法是使用实际输出和我们计算的输出之差。我们称之为错误。这里，我们不关心正负符号，因此我们取误差的绝对差值。因此，我们现在将使用错误更新表格。

<figure class="kg-card kg-image-card kg-width-full">![absolute difference of the error](../Images/a12594e807f14d9d481cbb967664c50c.png)</figure>

如果我们对这个误差求和，我们得到值 30。但是我们为什么要合计误差呢？因为我们要尝试多次猜测以得出最接近的答案，所以我们需要知道我们离前面的答案有多近或多远。这有助于我们完善我们的猜测并计算出正确的答案。

等等。但是如果我们只是把所有的误差值相加，感觉就像我们给了所有的答案同等的权重。我们不应该惩罚那些远远偏离目标的价值观吗？例如，这里的 10 比 2 高得多。正是在这里，我们引入了有点著名的“误差平方和”或简称 SSE。在 SSE 中，我们将所有的误差值平方，然后相加。因此，非常高的误差值被夸大了，这有助于我们了解如何进一步处理。

让我们把这些值放在下表中。

<figure class="kg-card kg-image-card kg-width-full">![error values](../Images/225a18b2c164d724e3b2e3510a276bb5.png)</figure>

权重为 5 的上证综指(记得我们假设 y = 5x)是 145。我们称之为损失函数。损失函数对于理解神经网络的效率是重要的，并且当我们将[反向传播](/backpropagation/)结合到神经网络中时，也有助于我们。

好了，到目前为止，我们了解了神经网络如何试图学习的原理。我们也看到了神经元的基本原理。现在让我们来理解神经网络本身的前向传播。

* * *

## 正向传播模型的组件

<figure class="kg-card kg-image-card">![Components of forward propagation model](../Images/9e2937f6eebf16b93b21f763e46d9acc.png)</figure>

在上图中，我们看到一个由三层组成的神经网络。第一和第三层是简单的输入和输出层。但是这个中间层是什么，为什么叫隐藏层呢？

现在，在我们的例子中，我们只有一个方程，因此每层只有一个神经元。

然而，隐藏层包含两个功能:

*   <bold>预激活功能</bold>:该功能计算输入的加权和。
*   <bold>激活函数</bold>:这里，基于加权和，应用激活函数来使网络非线性化，并使其随着计算的进行而学习。激活函数使用偏置使其非线性。

这就是关于神经网络中的前向传播的所有知识。但是等等！如何在交易中应用这个模型？下面就来了解一下。

* * *

## 正向传播的应用

在本例中，我们将使用 3 层网络(2 个输入单元、2 个隐藏层单元和 2 个输出单元)。网络和参数(或权重)可以表示如下。

<figure class="kg-card kg-image-card">![Applications of forward propagation](../Images/9e2937f6eebf16b93b21f763e46d9acc.png)</figure>

假设我们想训练这个神经网络来预测市场是上涨还是下跌。为此，我们将两个类指定为类 0 和类 1。

这里，类 0 表示市场收盘时的数据点，相反，类 1 表示市场收盘时的数据点。为了进行这种预测，训练数据(X)包括两个特征 x1 和 x2。

这里 x1 代表收盘价和收盘价的 10 天简单移动平均线(SMA)之间的相关性，x2 指收盘价和 10 天 SMA 之间的差异。

在下面的例子中，数据点属于类 1。输入数据的数学表示如下:

*X = [x1，x2] = [0.85，. 25] y= [1]*

具有两个数据点的示例:

$$ X = \begin{bmatrix} x_{11} & x_{12} \\ x_{22} & x_{22} \\ \end{bmatrix} = \begin{bmatrix} 0.85 & 0.25 \\ 0.71 & 0.29 \\ \end{bmatrix} $$$$ Y = \begin{bmatrix} y_1 \\ y_2 \\ \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ \end{bmatrix} $$

模型的输出是分类的或离散的数字。我们还需要将这个输出数据转换成矩阵形式。这使得模型能够预测数据点属于不同类别的概率。当我们进行这种矩阵转换时，列表示该示例所属的类，行表示每个输入示例。

$$ Y = \begin{bmatrix} y_1 \\ y_2 \\ \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \\ \end{bmatrix} $$

在矩阵 y 中，第一列表示类别 0，第二列表示类别 1。因为我们的例子属于类 1，我们在第二列中有 1，在第一列中有 0。

<figure class="kg-card kg-image-card">![class 0 class 1](../Images/49eb9e3f110c3c87cac740aa2ca63dfd.png)</figure>

这种将离散/分类类别转换为逻辑向量/矩阵的过程称为一键编码。这有点像转换十进制系统(1，2，3，4....9)到二进制(0，1，01，10，11)。我们使用一次性编码，因为神经网络不能直接对标签数据进行操作。它们要求所有输入变量和输出变量都是数字。

在一个[神经网络学习](https://quantra.quantinsti.com/course/neural-networks-deep-learning-trading-ernest-chan)中，除了输入变量之外，我们给除输出层之外的每一层增加一个偏置项。这个偏置项是一个常数，通常初始化为 1。该偏置使得能够沿着 x 轴移动激活阈值。

<figure class="kg-card kg-image-card">![forward propagation 6](../Images/48036389639f6510194d083d996c4310.png)</figure>

<figure class="kg-card kg-image-card">![forward propagation 7](../Images/c795d21263c54ccef427f67f46450626.png)</figure>

当偏差为负时，向右侧移动，当偏差为正时，向左侧移动。因此，一个有偏见的神经元应该能够学习，甚至是一个无偏见的神经元无法学习的输入向量。在数据集 X 中，为了引入这种偏差，我们添加了一个由 1 表示的新列，如下所示。

$$ X = \begin{bmatrix} x_0 & x_1 & x_2 \\ \end{bmatrix} = \begin{bmatrix} 1 & 0.85 & 0.25 \\ \end{bmatrix} $$

让我们随机初始化第一层中每个神经元的权重或参数。正如你在图中看到的，我们有一条线连接第一层的每个细胞和第二层的两个神经元。这给了我们总共 6 个要初始化的权重，隐藏层中的每个神经元 3 个。我们将这些权重表示如下。

$$ Theta_1 = \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ \end{bmatrix} $$

这里，θ<sub>1</sub>是对应于第一层的权重矩阵。

<figure class="kg-card kg-image-card">![forward propagation 11](../Images/0fd1bd1e34be834e29cd375d325a3cf1.png)</figure>

以上表示中的第一行示出了对应于第二层中的第一神经元的权重，第二行表示对应于第二层中的第二神经元的权重。现在，让我们进行前向传播的第一步，将每个示例的输入值乘以它们相应的权重，数学上如下所示。

θ<sub>1</sub>* X

在我们继续乘法之前，我们必须记住，当你做矩阵乘法时，乘积 X *θ的每个元素是第一个矩阵 X 的行与第二个矩阵θ1 的每个列的点积和。

当我们将两个矩阵 X 和θ1 相乘时，我们需要将权重与相应的输入示例值相乘。这意味着我们需要转置示例输入数据 X 的矩阵，以便矩阵将每个权重与相应的输入正确相乘。

$$ X_t = \begin{bmatrix} 1 \\ 0.85 \\ 0.25 \\ \end{bmatrix} $$

z<sup>2</sup>=θ<sub>1</sub>* X<sub>t</sub>

这里 z <sup>2</sup> 是矩阵相乘后的输出，X <sub>t</sub> 是 X的转置

矩阵乘法过程:

$$ \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ \end{bmatrix} * \begin{bmatrix} 1 \\ 0.85 \\ 0.25 \\ \end{bmatrix} $$ $$ = \begin{bmatrix} 0.1*1 + 0.2*0.85 + 0.3*0.25 \\ 0.4*1 + 0.5*0.85 + 0.6*0.25 \\ \end{bmatrix} = \begin{bmatrix} 1.02 \\ 0.975 \\ \end{bmatrix} $$

假设我们在输入层之后应用了 sigmoid 激活。然后，我们必须对上面 z 矩阵中的元素应用 sigmoid 函数。sigmoid 函数由以下等式给出:

$$ f(x) = \frac{1}{1+e^{-x}} $$

应用激活函数后，我们剩下一个 2×1 的矩阵，如下所示。

$$ a^{(2)} = \begin{bmatrix} 0.735 \\ 0.726 \\ \end{bmatrix} $$

这里 a <sup>(2)</sup> 表示激活层的输出。

激活层的这些输出充当下一层或最后一层(即输出层)的输入。让我们为隐藏层初始化另一个随机权重/参数，称为θ<sub>2</sub>。θ<sub>2</sub>中的每一行代表对应于输出层中两个神经元的权重。

$$ Theta_2 \begin{bmatrix} 0.5 & 0.4 & 0.3 \\ 0.2 & 0.5 & 0.1 \\ \end{bmatrix} $$

在初始化权重(Theta <sub>2</sub> 之后，我们将重复输入层的相同过程。我们将为前一层的输入增加一个偏置项。添加偏置向量后，a <sup>(2)</sup> 矩阵如下所示:

$$ a^{(2)} = \begin{bmatrix} 1 \\ 0.735 \\ 0.726 \\ \end{bmatrix} $$

让我们看看添加偏置单元后神经网络的样子:

<figure class="kg-card kg-image-card kg-width-wide">![forward propagation 17](../Images/65ff84b5eedab8eaebcd5f0f562dda5e.png)</figure>

在我们运行矩阵乘法来计算最终输出 z 之前，请记住，在 z 计算之前，我们必须转置输入数据 a，使其正确“排列”,以便矩阵乘法产生我们想要的计算结果。在这里，我们的矩阵已经按照我们想要的方式排列好了，所以不需要对 a <sup>(2)</sup> 矩阵进行转置。为了清楚地理解这一点，问自己这样一个问题:“哪些权重与哪些输入相乘？”。现在，让我们执行矩阵乘法:

z<sup>3</sup>=θ<sub>2</sub>* a<sup>(2)</sup>其中 z <sup>3</sup> 是应用激活函数之前的输出矩阵。

对于最后一层，我们将把一个 2x3 乘以一个 3x1 矩阵，得到一个输出假设的 2x1 矩阵。数学计算如下所示:

$$ \begin{bmatrix} 0.5 & 0.4 & 0.3 \\ 0.2 & 0.5 & 0.1 \\ \end{bmatrix} * \begin{bmatrix} 1 \\ 0.735 \\ 0.726 \\ \end{bmatrix} $$ $$ = \begin{bmatrix} 0.5*1 + 0.4*0.735 + 0.3*0.726 \\ 0.2*1 + 0.5*0.735 + 0.1*0.726 \\ \end{bmatrix} = \begin{bmatrix} 1.0118 \\ 0.6401 \\ \end{bmatrix} $$

在此乘法之后，在获得最终层的输出之前，我们使用 z 矩阵上的 sigmoid 函数进行元素转换。

a<sup>3</sup>= s 形(z <sup>3</sup>

其中 a <sup>3</sup> 表示最终输出矩阵。$ $ a^3 = \ begin { b matrix } 0.7333 \ \ 0.6548 \ \ \ end { b matrix } $ $

sigmoid 函数的输出是给定示例属于特定类别的概率。在上面的表示中，第一行表示该示例属于类 0 的概率，第二行表示类 1 的概率。

## 结论

如您所见，该示例属于 1 类的概率小于 0 类，这是不正确的，需要改进。因此，我们不仅理解了神经元的基本结构，也理解了神经网络。此外，我们借助于一个例子来研究神经网络中的前向传播。

在下一篇[博客](/backpropagation/)中，我们将讨论如何实现反向传播来减少预测中的误差。如果你想学习如何在交易中应用神经网络，那么请查看我们的新课程[神经网络&陈博士](https://quantra.quantinsti.com/course/neural-networks-deep-learning-trading-ernest-chan)的《交易中的深度学习》。

<small>免责声明:股票市场的所有投资和交易都有风险。在金融市场进行交易的任何决定，包括股票或期权或其他金融工具的交易，都是个人决定，只能在彻底研究后做出，包括个人风险和财务评估以及在您认为必要的范围内寻求专业帮助。本文提到的交易策略或相关信息仅供参考。</small>